{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7c463c-d3f2-4a53-a01b-0052d833eacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# From Speech to Answers with Whisper + Agent Bricks\n",
    "\n",
    "This notebook demonstrates a practical **speech-to-text** workflow using a Whisper serving endpoint, and then shows how the resulting transcript can be used as input to an **Agent Bricks Knowledge Assistant**.\n",
    "\n",
    "The goal is to take real voice interactions (e.g., customer calls, voicemails, field notes) and quickly turn them into text that can be summarized, routed, or answered using a RAG-powered assistant—without needing to manually structure the data first.\n",
    "\n",
    "## Key benefits include\n",
    "- **Fast speech-to-text with Whisper:** Convert audio (mp3/m4a/wav) into a clean transcript using a production serving endpoint.\n",
    "- **Seamless integration with Agent Bricks:** Use the transcript as the prompt/context for a Knowledge Assistant to generate helpful answers and next steps.\n",
    "- **Works with governed storage:** Read audio directly from Unity Catalog Volumes for secure, auditable workflows.\n",
    "- **Reusable pattern:** Parameterized endpoints + audio paths make it easy to apply the same workflow to many recordings.\n",
    "\n",
    "# Demo Overview\n",
    "\n",
    "For this demo, we use:\n",
    "1. **Audio clips** stored in a Unity Catalog Volume (simulating a customer voice interaction)\n",
    "2. A **Whisper** model serving endpoint to generate the transcript\n",
    "3. An **Agent Bricks Knowledge Assistant** endpoint to turn that transcript into an actionable response\n",
    "\n",
    "Workflow:\n",
    "1. Load audio bytes from `/Volumes/<catalog>/<schema>/audio/...`\n",
    "2. Call the **Whisper endpoint** to generate a transcript\n",
    "3. Call the **Knowledge Assistant endpoint** using the transcript as input\n",
    "\n",
    "By the end of this notebook, you’ll have an end-to-end pipeline that returns:\n",
    "- **Transcript**: the speech-to-text output from Whisper\n",
    "- **Answer**: a Knowledge Assistant response based on the transcript (for example: summary + recommended actions)\n",
    "\n",
    "This pattern is a strong starting point for voice-driven support automation, call summarization, ticket creation, and knowledge-base grounded Q&A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ba14fd-0929-45d9-b3dc-2fc8da6feae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1 — Install dependencies\n",
    "We’ll install the Databricks SDK used to call Model Serving endpoints from Python. After installing, we restart Python so the notebook uses the updated packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee4b67c-405b-4ecc-9537-db4af644559f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U databricks-sdk openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e007143d-8131-474c-abb9-3e5d99c7d7e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2 — Configure endpoints and audio input\n",
    "Here we set the Whisper endpoint name and the Knowledge Assistant endpoint name.  \n",
    "We also set the audio path (stored in a Unity Catalog Volume) and expose it as a widget so you can easily switch files without editing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916fe76b-bad3-4e21-a091-c61d80df4361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import DataframeSplitInput\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "WHISPER_ENDPOINT = \"whisper\"\n",
    "KNOWLEDGE_ASSISTANT_ENDPOINT = \"ka-87f4a3bd-endpoint\"\n",
    "AUDIO_PATH = \"/**/audio.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4efda0f4-4599-47a9-9892-37688621d85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3 — File helpers to read audio from Volumes, DBFS, or local paths\n",
    "Before calling Whisper, we need the raw audio bytes.  \n",
    "This cell provides small utilities to normalize different Databricks path formats (like `dbfs:/`, `file:/`, or `/Volumes/...`) into something Python can `open()`, then reads the audio file into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3449c9-a0e6-46e9-b5a2-b7270d7384ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# File helpers (DBFS / Volumes / local)\n",
    "# =========================\n",
    "def _normalize_path(path: str) -> str:\n",
    "    # allow dbfs:/... paths\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        return \"/dbfs/\" + path[len(\"dbfs:/\"):].lstrip(\"/\")\n",
    "    # allow file:/... paths\n",
    "    if path.startswith(\"file:/\"):\n",
    "        return path[len(\"file:\"):]\n",
    "    # /Volumes/... and local paths work as-is\n",
    "    return path\n",
    "\n",
    "def read_audio_bytes(audio_path: str) -> bytes:\n",
    "    p = _normalize_path(audio_path)\n",
    "    with open(p, \"rb\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1087ad41-542a-432a-96f8-1251a30d2290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4 — Transcribe audio with Whisper and extract the transcript\n",
    "This cell handles the core speech-to-text step.\n",
    "\n",
    "- We base64-encode the audio bytes and send them to the Whisper serving endpoint.\n",
    "- Because endpoint input signatures can vary, we try a few common request formats:\n",
    "  1) `dataframe_split` with a positional column **0** (int) — matches models logged with an input like `[0: binary]`\n",
    "  2) `dataframe_split` with column `\"0\"` (string) — sometimes used depending on how the model was logged\n",
    "  3) `inputs=[...]` — a tensor-style fallback\n",
    "\n",
    "After the endpoint returns, `extract_transcript()` normalizes the response into a single transcript string (handling common shapes like `predictions[0][\"text\"]`, `predictions[0][\"transcript\"]`, or `predictions[0]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc19a22a-0e6a-4b13-8a87-43bf68ec027c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Whisper endpoint call\n",
    "# =========================\n",
    "def call_whisper_endpoint(w: WorkspaceClient, audio_bytes: bytes) -> Any:\n",
    "    audio_b64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # IMPORTANT:\n",
    "    # Your model signature expects input [0: binary], where 0 is *positional* (numeric).\n",
    "    # So we must send columns=[0] (int), not [\"0\"] (str).\n",
    "    try:\n",
    "        return w.serving_endpoints.query(\n",
    "            name=WHISPER_ENDPOINT,\n",
    "            dataframe_split=DataframeSplitInput(\n",
    "                columns=[0],          # <-- int (fixes missing [0] / extra ['0'])\n",
    "                data=[[audio_b64]],\n",
    "            ),\n",
    "        )\n",
    "    except Exception as e1:\n",
    "        # Fallback 1: some models are logged expecting string \"0\"\n",
    "        try:\n",
    "            return w.serving_endpoints.query(\n",
    "                name=WHISPER_ENDPOINT,\n",
    "                dataframe_split=DataframeSplitInput(\n",
    "                    columns=[\"0\"],\n",
    "                    data=[[audio_b64]],\n",
    "                ),\n",
    "            )\n",
    "        except Exception as e2:\n",
    "            # Fallback 2: some endpoints are tensor-style; try `inputs`\n",
    "            # (If your SDK is very old, this might raise \"unexpected keyword argument 'inputs'\")\n",
    "            try:\n",
    "                return w.serving_endpoints.query(\n",
    "                    name=WHISPER_ENDPOINT,\n",
    "                    inputs=[audio_b64],\n",
    "                )\n",
    "            except Exception as e3:\n",
    "                raise RuntimeError(\n",
    "                    \"Failed calling Whisper endpoint with dataframe_split (int 0), \"\n",
    "                    \"dataframe_split (str '0'), and inputs[].\\n\\n\"\n",
    "                    f\"Error 1 (int 0): {e1}\\n\\n\"\n",
    "                    f\"Error 2 (str '0'): {e2}\\n\\n\"\n",
    "                    f\"Error 3 (inputs[]): {e3}\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "def extract_transcript(whisper_query_resp: Any) -> str:\n",
    "    preds = getattr(whisper_query_resp, \"predictions\", None)\n",
    "    if not preds:\n",
    "        # Sometimes response might be dict-like\n",
    "        if isinstance(whisper_query_resp, dict) and \"predictions\" in whisper_query_resp:\n",
    "            preds = whisper_query_resp[\"predictions\"]\n",
    "        else:\n",
    "            raise RuntimeError(f\"No predictions found in Whisper response: {whisper_query_resp}\")\n",
    "\n",
    "    pred0 = preds[0]\n",
    "\n",
    "    if isinstance(pred0, dict):\n",
    "        if isinstance(pred0.get(\"text\"), str):\n",
    "            return pred0[\"text\"]\n",
    "        if isinstance(pred0.get(\"transcript\"), str):\n",
    "            return pred0[\"transcript\"]\n",
    "        return str(pred0)\n",
    "\n",
    "    if isinstance(pred0, str):\n",
    "        return pred0\n",
    "\n",
    "    return str(pred0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7c037d-a49a-4983-a898-c5682aaf63d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5 — Send the transcript to the Knowledge Assistant and extract a clean answer\n",
    "Now that we have speech converted into text, we use that transcript as the input to an Agent Bricks **Knowledge Assistant** endpoint.\n",
    "\n",
    "This cell includes:\n",
    "- `call_knowledge_assistant()`: calls the endpoint using the OpenAI-compatible client from Databricks Model Serving.  \n",
    "  It tries the newer **Responses API** first, and falls back to **Chat Completions** if needed (different workspaces/endpoints can vary).\n",
    "- `extract_agent_text()`: pulls out a readable answer string from whichever response format is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99861b2f-2d7f-4095-963a-7ccb544911d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Knowledge Assistant call\n",
    "# =========================\n",
    "def extract_agent_text(resp: Any) -> str:\n",
    "    # Best case\n",
    "    if hasattr(resp, \"output_text\") and resp.output_text:\n",
    "        return resp.output_text\n",
    "\n",
    "    # Responses API fallback\n",
    "    try:\n",
    "        parts = []\n",
    "        for item in resp.output:\n",
    "            for c in item.content:\n",
    "                if getattr(c, \"text\", None):\n",
    "                    parts.append(c.text)\n",
    "        if parts:\n",
    "            return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Chat Completions fallback\n",
    "    try:\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return str(resp)\n",
    "\n",
    "\n",
    "def call_knowledge_assistant(w: WorkspaceClient, user_text: str) -> Any:\n",
    "    client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "    # Try Responses API first\n",
    "    try:\n",
    "        return client.responses.create(\n",
    "            model=KNOWLEDGE_ASSISTANT_ENDPOINT,\n",
    "            input=[{\"role\": \"user\", \"content\": user_text}],\n",
    "        )\n",
    "    except Exception:\n",
    "        # Fallback to Chat Completions\n",
    "        return client.chat.completions.create(\n",
    "            model=KNOWLEDGE_ASSISTANT_ENDPOINT,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_text}],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a7bc4b-0cc4-4c31-895c-09eda0abf3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6 — Run the full speech-to-answer pipeline\n",
    "This final cell ties everything together into a single workflow:\n",
    "\n",
    "1. Read the audio file from the configured path (`AUDIO_PATH`)\n",
    "2. Transcribe the audio with the Whisper endpoint to produce a **transcript**\n",
    "3. Send the transcript to the Knowledge Assistant endpoint to generate an **answer**\n",
    "4. Print the transcript and the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f58710-2805-423e-a2ac-576d889549bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# End-to-end runner\n",
    "# =========================\n",
    "def speech_to_answer(audio_path: str) -> Dict[str, Any]:\n",
    "    w = WorkspaceClient()\n",
    "\n",
    "    audio_bytes = read_audio_bytes(audio_path)\n",
    "\n",
    "    whisper_resp = call_whisper_endpoint(w, audio_bytes)\n",
    "    transcript = extract_transcript(whisper_resp)\n",
    "`   ssistant(w, transcript)\n",
    "    answer = extract_agent_text(agent_resp)\n",
    "\n",
    "    return {\n",
    "        \"audio_path\": audio_path,\n",
    "        \"transcript\": transcript,\n",
    "        \"answer\": answer,\n",
    "        \"whisper_raw\": whisper_resp,\n",
    "        \"agent_raw\": agent_resp,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "result = speech_to_answer(AUDIO_PATH) \n",
    "\n",
    "print(\"===== TRANSCRIPT =====\")\n",
    "print(result[\"transcript\"])\n",
    "print(\"\\n===== ANSWER =====\")\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5555390509215094,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "06_Voice_Whisper_AgentBricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
