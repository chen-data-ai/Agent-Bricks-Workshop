{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16f2c19-3e85-4fc5-9c75-7fec8edda95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Setup\n",
    "\n",
    "Before running this notebook, update the `config.py` file with your catalog and schema names.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7330664f-e46e-4587-8f10-e30fd91dadf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Run this cell to initialize resource names. You can edit resource names in the config file before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc0ecad-3364-4142-ae1a-926b9d6feb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eaf3818-6514-432d-84d3-061314a82cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run this section to import data files from repo into tables/volumes under your specified catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf46e237-09be-473e-b403-047b3677c393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.workspace import ObjectType, ExportFormat\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def ensure_catalog_and_schema(spark, catalog: str, schema: str):\n",
    "    \"\"\"\n",
    "    Ensure the Unity Catalog schema exists.\n",
    "\n",
    "    Note:\n",
    "    - We assume the catalog already exists / is managed outside this workshop.\n",
    "    - If you want the workshop to create a dedicated catalog, uncomment the CREATE CATALOG line.\n",
    "    \"\"\"\n",
    "    # If you want to create a new catalog for this workshop:\n",
    "    # spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    logger.info(f\"Ensured catalog `{catalog}` and schema `{schema}` exist.\")\n",
    "\n",
    "def ensure_volume(spark, catalog: str, schema: str, volume: str):\n",
    "    \"\"\"\n",
    "    Ensure a Unity Catalog volume exists.\n",
    "    Volumes provide a governed storage location for unstructured data under UC.\n",
    "    \"\"\"\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume}`\")\n",
    "    logger.info(f\"Ensured volume `{catalog}`.`{schema}`.`{volume}` exists.\")\n",
    "\n",
    "def read_workspace_csv_to_pandas(w: WorkspaceClient, workspace_path: str) -> pd.DataFrame:\n",
    "    exported = w.workspace.export(workspace_path, format=ExportFormat.AUTO)\n",
    "    if not hasattr(exported, \"content\") or exported.content is None:\n",
    "        raise RuntimeError(f\"Export returned no content for {workspace_path}\")\n",
    "    content_bytes = base64.b64decode(exported.content)\n",
    "    return pd.read_csv(io.BytesIO(content_bytes))\n",
    "\n",
    "def load_workspace_csv_to_delta_table(\n",
    "    spark,\n",
    "    w: WorkspaceClient,\n",
    "    table: str,\n",
    "    workspace_csv_path: str,\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    mode: str = \"overwrite\"\n",
    "):\n",
    "    full_table = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "    logger.info(f\"Loading {full_table} from {workspace_csv_path}\")\n",
    "\n",
    "    pdf = read_workspace_csv_to_pandas(w, workspace_csv_path)\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "    (sdf.write\n",
    "        .format(\"delta\")\n",
    "        .mode(mode)\n",
    "        .option(\"overwriteSchema\", \"true\" if mode == \"overwrite\" else \"false\")\n",
    "        .saveAsTable(full_table))\n",
    "\n",
    "    logger.info(f\"Wrote {full_table} ({mode}).\")\n",
    "\n",
    "def copy_folder_to_volume(w: WorkspaceClient, source_folder: str, destination_folder: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Recursively copy a folder from the Databricks Workspace to a UC Volume.\n",
    "\n",
    "    source_folder:\n",
    "      Workspace path like /Workspace/Users/.../data/tech_support/knowledge_base\n",
    "    destination_folder:\n",
    "      Volume path like /Volumes/<catalog>/<schema>/<volume>\n",
    "\n",
    "    Implementation details:\n",
    "    - Uses workspace.list() to traverse folders/files\n",
    "    - Uses workspace.export() to fetch file bytes (base64 content)\n",
    "    - Writes files into the volume using normal Python file I/O\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Copying from {source_folder} to {destination_folder}\")\n",
    "\n",
    "    items = w.workspace.list(source_folder)\n",
    "\n",
    "    for item in items:\n",
    "        item_name = item.path.split(\"/\")[-1]\n",
    "        dest_path = f\"{destination_folder}/{item_name}\"\n",
    "\n",
    "        if item.object_type == ObjectType.DIRECTORY:\n",
    "            os.makedirs(dest_path, exist_ok=True)\n",
    "            if verbose:\n",
    "                print(f\"Created folder: {dest_path}\")\n",
    "            copy_folder_to_volume(w, item.path, dest_path, verbose)\n",
    "\n",
    "        elif item.object_type == ObjectType.FILE:\n",
    "            exported = w.workspace.export(item.path, format=ExportFormat.AUTO)\n",
    "            if not hasattr(exported, \"content\") or exported.content is None:\n",
    "                raise RuntimeError(f\"Export returned no content for {item.path}\")\n",
    "            content = base64.b64decode(exported.content)\n",
    "\n",
    "            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "            with open(dest_path, \"wb\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Copied file: {item_name}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Completed copying {source_folder}\")\n",
    "\n",
    "def main(spark):\n",
    "    \"\"\"\n",
    "    Main workshop setup:\n",
    "    1) Resolve the workspace data folder relative to this notebook location\n",
    "    2) Ensure UC schema exists\n",
    "    3) Load CSV datasets into UC Delta tables\n",
    "    4) Create volumes for unstructured data\n",
    "    5) Copy unstructured folders into volumes\n",
    "    \"\"\"\n",
    "    catalog = catalog_name\n",
    "    schema  = schema_name\n",
    "\n",
    "    w = WorkspaceClient()\n",
    "\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    notebook_dir = os.path.dirname(notebook_path)\n",
    "    logger.info(f\"Notebook path: {notebook_path}\")\n",
    "    logger.info(f\"Notebook dir:  {notebook_dir}\")\n",
    "\n",
    "    ws_data_folder = f\"/Workspace{notebook_dir}/data\"\n",
    "    logger.info(f\"Workspace data folder: {ws_data_folder}\")\n",
    "\n",
    "    ensure_catalog_and_schema(spark, catalog, schema)\n",
    "\n",
    "    csv_files = {\n",
    "        \"billing\": f\"{ws_data_folder}/billing.csv\",\n",
    "        \"customers\": f\"{ws_data_folder}/customers.csv\",\n",
    "        \"knowledge_base\": f\"{ws_data_folder}/knowledge_base.csv\",\n",
    "        \"support_tickets\": f\"{ws_data_folder}/support_tickets.csv\",\n",
    "        \"cust_service_data\": f\"{ws_data_folder}/cust_service_data.csv\",\n",
    "        \"policies\": f\"{ws_data_folder}/policies.csv\",\n",
    "        \"product_docs\": f\"{ws_data_folder}/product_docs.csv\",\n",
    "        \"promotions\": f\"{ws_data_folder}/promotions.csv\",\n",
    "        \"devices\": f\"{ws_data_folder}/devices.csv\",\n",
    "        \"plans\": f\"{ws_data_folder}/plans.csv\",\n",
    "    }\n",
    "\n",
    "    for tbl, ws_csv_path in csv_files.items():\n",
    "        load_workspace_csv_to_delta_table(\n",
    "            spark=spark,\n",
    "            w=w,\n",
    "            table=tbl,\n",
    "            workspace_csv_path=ws_csv_path,\n",
    "            catalog=catalog,\n",
    "            schema=schema,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "\n",
    "    volume_kb = \"knowledge_base\"\n",
    "    volume_st = \"support_tickets\"\n",
    "\n",
    "    ensure_volume(spark, catalog, schema, volume_kb)\n",
    "    ensure_volume(spark, catalog, schema, volume_st)\n",
    "\n",
    "    kb_src = f\"{ws_data_folder}/tech_support/knowledge_base\"\n",
    "    st_src = f\"{ws_data_folder}/tech_support/support_tickets\"\n",
    "\n",
    "    kb_dst = f\"/Volumes/{catalog}/{schema}/{volume_kb}\"\n",
    "    st_dst = f\"/Volumes/{catalog}/{schema}/{volume_st}\"\n",
    "\n",
    "    copy_folder_to_volume(w, kb_src, kb_dst, verbose=True)\n",
    "    copy_folder_to_volume(w, st_src, st_dst, verbose=True)\n",
    "\n",
    "    # Sample invoice PDF\n",
    "    ensure_volume(spark, catalog, schema, 'invoice')\n",
    "    copy_folder_to_volume(w, f\"{ws_data_folder}/Invoice INV-2026-01482.pdf\", f\"/Volumes/{catalog}/{schema}/invoice\", verbose=True)\n",
    "    \n",
    "    # Audio volume for voice demos\n",
    "    volume_audio = \"audio\"\n",
    "    ensure_volume(spark, catalog, schema, volume_audio)\n",
    "\n",
    "    audio_src = f\"{ws_data_folder}/audio/audio.mp3\"  # Workspace folder: data/audio (contains .mp3)\n",
    "    audio_dst = f\"/Volumes/{catalog}/{schema}/{volume_audio}\"\n",
    "\n",
    "    copy_folder_to_volume(w, audio_src, audio_dst, verbose=True)\n",
    "\n",
    "    logger.info(\"Setup completed successfully.\")\n",
    "\n",
    "main(spark)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7358320039447319,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
